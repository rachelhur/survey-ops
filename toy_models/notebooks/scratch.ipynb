{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d5fd71",
   "metadata": {},
   "source": [
    "#### DQN_closest_distance_path env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has issues: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)\n"
     ]
    }
   ],
   "source": [
    "# Register the environment so we can create it with gym.make()\n",
    "gym.register(\n",
    "    id=f\"gymnasium_env/{env_name}\",\n",
    "    entry_point=ToyEnv_v2,\n",
    "    max_episode_steps=300,  # Prevent infinite episodes. Here just set to 300 even though episode will terminate when stepping to last element of sequence\n",
    ")\n",
    "env = gym.make(f\"gymnasium_env/{env_name}\", coords_dict=coords_dict, max_visits=1)\n",
    "# Create multiple environments for parallel training\n",
    "# vec_env = gym.make_vec(\"gymnasium_env/SimpleTel-v0\", num_envs=5, vectorization_mode='sync', Nf=Nf, target_sequence=true_sequence, nv_max=nv_max)\n",
    "\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# This will catch many common issues\n",
    "try:\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has issues: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85035f41",
   "metadata": {},
   "source": [
    "#### Sample framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Universal Offline Reinforcement Learning Framework\n",
    "Supports: DQN, DDPG, SAC, TD3, BC, CQL, IQL, and more\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Tuple, Optional, List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BASE DATASET (Same as before)\n",
    "# ============================================================================\n",
    "\n",
    "class BaseDataset(Dataset, ABC):\n",
    "    \"\"\"Abstract base class for offline RL datasets\"\"\"\n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __getitem__(self, idx: int) -> Tuple:\n",
    "        pass\n",
    "\n",
    "\n",
    "class OfflineDataset(BaseDataset):\n",
    "    \"\"\"Universal offline dataset for any RL algorithm\"\"\"\n",
    "    def __init__(self, states, actions, rewards, next_states, dones):\n",
    "        self.states = np.array(states, dtype=np.float32)\n",
    "        self.actions = np.array(actions, dtype=np.float32)\n",
    "        self.rewards = np.array(rewards, dtype=np.float32)\n",
    "        self.next_states = np.array(next_states, dtype=np.float32)\n",
    "        self.dones = np.array(dones, dtype=np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.next_states[idx],\n",
    "            self.dones[idx]\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. NETWORK COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[256, 256], \n",
    "                 activation='relu', output_activation=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        activations = {\n",
    "            'relu': nn.ReLU, 'tanh': nn.Tanh, \n",
    "            'elu': nn.ELU, 'leaky_relu': nn.LeakyReLU\n",
    "        }\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(activations[activation]())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        if output_activation:\n",
    "            layers.append(activations[output_activation]())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network for DQN variants (discrete actions)\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.q_net = MLP(state_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    def forward(self, states):\n",
    "        return self.q_net(states)\n",
    "\n",
    "\n",
    "class ContinuousQNetwork(nn.Module):\n",
    "    \"\"\"Q-Network for continuous actions (DDPG, TD3, SAC)\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.q_net = MLP(state_dim + action_dim, 1, hidden_dims)\n",
    "    \n",
    "    def forward(self, states, actions):\n",
    "        x = torch.cat([states, actions], dim=-1)\n",
    "        return self.q_net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    \"\"\"Deterministic policy for DDPG, TD3\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256], \n",
    "                 action_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.policy = MLP(state_dim, action_dim, hidden_dims, output_activation='tanh')\n",
    "        self.action_scale = action_scale\n",
    "    \n",
    "    def forward(self, states):\n",
    "        return self.policy(states) * self.action_scale\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    \"\"\"Gaussian policy for SAC, continuous BC\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256],\n",
    "                 log_std_min=-20, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.backbone = MLP(state_dim, hidden_dims[-1], hidden_dims[:-1])\n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_dims[-1], action_dim)\n",
    "    \n",
    "    def forward(self, states):\n",
    "        features = self.backbone(states)\n",
    "        mean = self.mean_layer(features)\n",
    "        log_std = self.log_std_layer(features)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, states):\n",
    "        mean, log_std = self.forward(states)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # Reparameterization trick\n",
    "        action = torch.tanh(x_t)\n",
    "        \n",
    "        # Compute log prob with tanh correction\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    \"\"\"Categorical policy for discrete BC\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.policy = MLP(state_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    def forward(self, states):\n",
    "        logits = self.policy(states)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "    \n",
    "    def sample(self, states):\n",
    "        probs = self.forward(states)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. BASE ALGORITHM INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "class BaseOfflineAlgorithm(ABC):\n",
    "    \"\"\"Abstract base class for all offline RL algorithms\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train_step(self, batch) -> Dict[str, float]:\n",
    "        \"\"\"Single training step, returns metrics\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def select_action(self, state, deterministic=True):\n",
    "        \"\"\"Select action for evaluation\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self, filepath):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load(self, filepath):\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DQN FAMILY ALGORITHMS\n",
    "# ============================================================================\n",
    "\n",
    "class DQNAlgorithm(BaseOfflineAlgorithm):\n",
    "    \"\"\"Standard DQN / Double DQN for discrete actions\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256],\n",
    "                 lr=3e-4, gamma=0.99, tau=0.005, double_q=True,\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.double_q = double_q\n",
    "        self.device = device\n",
    "        \n",
    "        self.q_net = QNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_q_net = QNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            torch.FloatTensor(x).to(self.device) for x in batch\n",
    "        ]\n",
    "        actions = actions.long()\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            if self.double_q:\n",
    "                next_actions = self.q_net(next_states).argmax(1)\n",
    "                next_q = self.target_q_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            else:\n",
    "                next_q = self.target_q_net(next_states).max(1)[0]\n",
    "            \n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Loss and optimization\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update target\n",
    "        for param, target_param in zip(self.q_net.parameters(), self.target_q_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        return {'loss': loss.item(), 'q_value': current_q.mean().item()}\n",
    "    \n",
    "    def select_action(self, state, deterministic=True):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({'q_net': self.q_net.state_dict()}, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_net.load_state_dict(checkpoint['q_net'])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. DDPG ALGORITHM (Continuous Control)\n",
    "# ============================================================================\n",
    "\n",
    "class DDPGAlgorithm(BaseOfflineAlgorithm):\n",
    "    \"\"\"Deep Deterministic Policy Gradient for continuous actions\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256],\n",
    "                 actor_lr=3e-4, critic_lr=3e-4, gamma=0.99, tau=0.005,\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "        \n",
    "        # Actor\n",
    "        self.actor = DeterministicPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_actor = DeterministicPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Critic\n",
    "        self.critic = ContinuousQNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_critic = ContinuousQNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            torch.FloatTensor(x).to(self.device) for x in batch\n",
    "        ]\n",
    "        \n",
    "        # Update Critic\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor(next_states)\n",
    "            target_q = self.target_critic(next_states, next_actions)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * target_q\n",
    "        \n",
    "        current_q = self.critic(states, actions)\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update Actor\n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update targets\n",
    "        self._soft_update(self.actor, self.target_actor)\n",
    "        self._soft_update(self.critic, self.target_critic)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'q_value': current_q.mean().item()\n",
    "        }\n",
    "    \n",
    "    def _soft_update(self, source, target):\n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def select_action(self, state, deterministic=True):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            action = self.actor(state)\n",
    "            return action.cpu().numpy()[0]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict()\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SAC ALGORITHM (Maximum Entropy RL)\n",
    "# ============================================================================\n",
    "\n",
    "class SACAlgorithm(BaseOfflineAlgorithm):\n",
    "    \"\"\"Soft Actor-Critic for continuous actions\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256],\n",
    "                 actor_lr=3e-4, critic_lr=3e-4, alpha=0.2, gamma=0.99, tau=0.005,\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.device = device\n",
    "        \n",
    "        # Actor (stochastic policy)\n",
    "        self.actor = GaussianPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Twin Critics\n",
    "        self.critic1 = ContinuousQNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic2 = ContinuousQNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_critic1 = ContinuousQNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.target_critic2 = ContinuousQNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "        \n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            list(self.critic1.parameters()) + list(self.critic2.parameters()), \n",
    "            lr=critic_lr\n",
    "        )\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            torch.FloatTensor(x).to(self.device) for x in batch\n",
    "        ]\n",
    "        \n",
    "        # Update Critics\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_states)\n",
    "            target_q1 = self.target_critic1(next_states, next_actions)\n",
    "            target_q2 = self.target_critic2(next_states, next_actions)\n",
    "            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs.squeeze()\n",
    "            target_q = rewards + (1 - dones) * self.gamma * target_q\n",
    "        \n",
    "        current_q1 = self.critic1(states, actions)\n",
    "        current_q2 = self.critic2(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update Actor\n",
    "        new_actions, log_probs = self.actor.sample(states)\n",
    "        q1 = self.critic1(states, new_actions)\n",
    "        q2 = self.critic2(states, new_actions)\n",
    "        q = torch.min(q1, q2)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_probs.squeeze() - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update\n",
    "        self._soft_update(self.critic1, self.target_critic1)\n",
    "        self._soft_update(self.critic2, self.target_critic2)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'q_value': current_q1.mean().item()\n",
    "        }\n",
    "    \n",
    "    def _soft_update(self, source, target):\n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def select_action(self, state, deterministic=True):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            if deterministic:\n",
    "                mean, _ = self.actor(state)\n",
    "                action = torch.tanh(mean)\n",
    "            else:\n",
    "                action, _ = self.actor.sample(state)\n",
    "            return action.cpu().numpy()[0]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({'actor': self.actor.state_dict()}, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. BEHAVIORAL CLONING (Imitation Learning)\n",
    "# ============================================================================\n",
    "\n",
    "class BehavioralCloningAlgorithm(BaseOfflineAlgorithm):\n",
    "    \"\"\"Behavioral Cloning - simple supervised learning\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256],\n",
    "                 lr=3e-4, discrete=True,\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.discrete = discrete\n",
    "        self.device = device\n",
    "        \n",
    "        if discrete:\n",
    "            self.policy = CategoricalPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        else:\n",
    "            self.policy = GaussianPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        states, actions, _, _, _ = [torch.FloatTensor(x).to(self.device) for x in batch]\n",
    "        \n",
    "        if self.discrete:\n",
    "            actions = actions.long()\n",
    "            probs = self.policy(states)\n",
    "            loss = F.cross_entropy(probs, actions)\n",
    "        else:\n",
    "            mean, log_std = self.policy(states)\n",
    "            std = log_std.exp()\n",
    "            loss = F.mse_loss(mean, actions)  # Simple MSE for continuous\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'loss': loss.item()}\n",
    "    \n",
    "    def select_action(self, state, deterministic=True):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            if self.discrete:\n",
    "                probs = self.policy(state)\n",
    "                return probs.argmax().item()\n",
    "            else:\n",
    "                mean, _ = self.policy(state)\n",
    "                return mean.cpu().numpy()[0]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({'policy': self.policy.state_dict()}, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.policy.load_state_dict(checkpoint['policy'])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. AGENT WRAPPER (High-Level Interface)\n",
    "# ============================================================================\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    High-level agent wrapper that provides a convenient interface for:\n",
    "    - Training\n",
    "    - Evaluation  \n",
    "    - Saving/Loading\n",
    "    - Action selection with exploration\n",
    "    - Logging and monitoring\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        algorithm: BaseOfflineAlgorithm,\n",
    "        env=None,\n",
    "        name: str = \"agent\"\n",
    "    ):\n",
    "        self.algorithm = algorithm\n",
    "        self.env = env\n",
    "        self.name = name\n",
    "        self.training_history = []\n",
    "        self.eval_history = []\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        dataset: BaseDataset,\n",
    "        num_epochs: int,\n",
    "        batch_size: int = 256,\n",
    "        log_interval: int = 10,\n",
    "        eval_interval: Optional[int] = None,\n",
    "        eval_episodes: int = 5,\n",
    "        save_best: bool = True,\n",
    "        save_dir: str = \"./checkpoints\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the agent with optional periodic evaluation\n",
    "        \n",
    "        Args:\n",
    "            dataset: Offline dataset\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Batch size\n",
    "            log_interval: Log every N epochs\n",
    "            eval_interval: Evaluate every N epochs (None = no eval)\n",
    "            eval_episodes: Number of episodes for evaluation\n",
    "            save_best: Save best model during training\n",
    "            save_dir: Directory to save checkpoints\n",
    "        \"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        trainer = UniversalTrainer(self.algorithm, dataset, batch_size)\n",
    "        \n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        print(f\"Training {self.name} for {num_epochs} epochs...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Train one epoch\n",
    "            epoch_metrics = {}\n",
    "            for batch in trainer.dataloader:\n",
    "                metrics = self.algorithm.train_step(batch)\n",
    "                for key, value in metrics.items():\n",
    "                    if key not in epoch_metrics:\n",
    "                        epoch_metrics[key] = []\n",
    "                    epoch_metrics[key].append(value)\n",
    "            \n",
    "            # Average metrics\n",
    "            avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n",
    "            self.training_history.append({'epoch': epoch, 'metrics': avg_metrics})\n",
    "            \n",
    "            # Logging\n",
    "            if (epoch + 1) % log_interval == 0:\n",
    "                metric_str = ', '.join([f\"{k}: {v:.4f}\" for k, v in avg_metrics.items()])\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs} | {metric_str}\")\n",
    "            \n",
    "            # Periodic evaluation\n",
    "            if eval_interval and (epoch + 1) % eval_interval == 0 and self.env:\n",
    "                print(f\"\\n--- Evaluation at epoch {epoch + 1} ---\")\n",
    "                avg_reward, std_reward = self.evaluate(\n",
    "                    num_episodes=eval_episodes,\n",
    "                    verbose=False\n",
    "                )\n",
    "                self.eval_history.append({\n",
    "                    'epoch': epoch,\n",
    "                    'avg_reward': avg_reward,\n",
    "                    'std_reward': std_reward\n",
    "                })\n",
    "                print(f\"Reward: {avg_reward:.2f} Â± {std_reward:.2f}\\n\")\n",
    "                \n",
    "                # Save best model\n",
    "                if save_best and avg_reward > best_reward:\n",
    "                    best_reward = avg_reward\n",
    "                    save_path = os.path.join(save_dir, f\"{self.name}_best.pt\")\n",
    "                    self.save(save_path)\n",
    "                    print(f\"ğŸ’¾ Saved best model (reward: {best_reward:.2f})\")\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Training complete!\")\n",
    "        \n",
    "        # Final save\n",
    "        final_path = os.path.join(save_dir, f\"{self.name}_final.pt\")\n",
    "        self.save(final_path)\n",
    "        print(f\"ğŸ’¾ Saved final model to {final_path}\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        env=None,\n",
    "        num_episodes: int = 10,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the agent\n",
    "        \n",
    "        Args:\n",
    "            env: Environment (uses self.env if None)\n",
    "            num_episodes: Number of episodes\n",
    "            deterministic: Use deterministic policy\n",
    "            render: Render environment\n",
    "            verbose: Print results\n",
    "        \n",
    "        Returns:\n",
    "            (avg_reward, std_reward)\n",
    "        \"\"\"\n",
    "        env = env or self.env\n",
    "        if env is None:\n",
    "            raise ValueError(\"No environment provided for evaluation\")\n",
    "        \n",
    "        evaluator = UniversalEvaluator(self.algorithm, env)\n",
    "        \n",
    "        if verbose:\n",
    "            return evaluator.evaluate(num_episodes, deterministic)\n",
    "        else:\n",
    "            # Silent evaluation\n",
    "            total_rewards = []\n",
    "            for _ in range(num_episodes):\n",
    "                state, _ = env.reset()\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    action = self.algorithm.select_action(state, deterministic)\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                \n",
    "                total_rewards.append(episode_reward)\n",
    "            \n",
    "            return np.mean(total_rewards), np.std(total_rewards)\n",
    "    \n",
    "    def act(self, state, deterministic: bool = True, epsilon: float = 0.0):\n",
    "        \"\"\"\n",
    "        Select action with optional epsilon-greedy exploration\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            deterministic: Use deterministic policy\n",
    "            epsilon: Exploration probability (for epsilon-greedy)\n",
    "        \n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy exploration (mainly for DQN)\n",
    "        if epsilon > 0 and np.random.random() < epsilon:\n",
    "            # Random action\n",
    "            if hasattr(self.env.action_space, 'n'):\n",
    "                # Discrete\n",
    "                return np.random.randint(0, self.env.action_space.n)\n",
    "            else:\n",
    "                # Continuous\n",
    "                return self.env.action_space.sample()\n",
    "        \n",
    "        return self.algorithm.select_action(state, deterministic)\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save agent to file\"\"\"\n",
    "        self.algorithm.save(filepath)\n",
    "        print(f\"Agent saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Load agent from file\"\"\"\n",
    "        self.algorithm.load(filepath)\n",
    "        print(f\"Agent loaded from {filepath}\")\n",
    "    \n",
    "    def get_training_history(self):\n",
    "        \"\"\"Get training history as dict\"\"\"\n",
    "        return self.training_history\n",
    "    \n",
    "    def get_eval_history(self):\n",
    "        \"\"\"Get evaluation history as dict\"\"\"\n",
    "        return self.eval_history\n",
    "    \n",
    "    def plot_training(self):\n",
    "        \"\"\"Plot training curves (requires matplotlib)\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            if not self.training_history:\n",
    "                print(\"No training history to plot\")\n",
    "                return\n",
    "            \n",
    "            # Extract metrics\n",
    "            epochs = [h['epoch'] for h in self.training_history]\n",
    "            metrics = self.training_history[0]['metrics'].keys()\n",
    "            \n",
    "            fig, axes = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 4))\n",
    "            if len(metrics) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for ax, metric in zip(axes, metrics):\n",
    "                values = [h['metrics'][metric] for h in self.training_history]\n",
    "                ax.plot(epochs, values)\n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "                ax.set_title(f'{metric.replace(\"_\", \" \").title()} over Training')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{self.name}_training.png\", dpi=150, bbox_inches='tight')\n",
    "            print(f\"Training plot saved to {self.name}_training.png\")\n",
    "            plt.show()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"matplotlib not installed. Cannot plot training curves.\")\n",
    "    \n",
    "    def plot_evaluation(self):\n",
    "        \"\"\"Plot evaluation curves (requires matplotlib)\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            if not self.eval_history:\n",
    "                print(\"No evaluation history to plot\")\n",
    "                return\n",
    "            \n",
    "            epochs = [h['epoch'] for h in self.eval_history]\n",
    "            rewards = [h['avg_reward'] for h in self.eval_history]\n",
    "            stds = [h['std_reward'] for h in self.eval_history]\n",
    "            \n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(epochs, rewards, marker='o', label='Average Reward')\n",
    "            plt.fill_between(epochs, \n",
    "                           np.array(rewards) - np.array(stds),\n",
    "                           np.array(rewards) + np.array(stds),\n",
    "                           alpha=0.3, label='Â±1 Std Dev')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.title('Evaluation Performance')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{self.name}_evaluation.png\", dpi=150, bbox_inches='tight')\n",
    "            print(f\"Evaluation plot saved to {self.name}_evaluation.png\")\n",
    "            plt.show()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"matplotlib not installed. Cannot plot evaluation curves.\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Agent(name={self.name}, algorithm={self.algorithm.__class__.__name__})\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. UNIVERSAL TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "class UniversalTrainer:\n",
    "    \"\"\"Universal trainer for any offline RL algorithm\"\"\"\n",
    "    def __init__(self, algorithm: BaseOfflineAlgorithm, dataset: BaseDataset,\n",
    "                 batch_size: int = 256, num_workers: int = 0):\n",
    "        self.algorithm = algorithm\n",
    "        self.dataloader = DataLoader(\n",
    "            dataset, batch_size=batch_size, \n",
    "            shuffle=True, num_workers=num_workers\n",
    "        )\n",
    "    \n",
    "    def train(self, num_epochs: int, log_interval: int = 10):\n",
    "        \"\"\"Train algorithm on offline dataset\"\"\"\n",
    "        print(f\"Training for {num_epochs} epochs...\")\n",
    "        \n",
    "        history = {'epoch': [], 'metrics': []}\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = {}\n",
    "            \n",
    "            for batch in self.dataloader:\n",
    "                metrics = self.algorithm.train_step(batch)\n",
    "                \n",
    "                for key, value in metrics.items():\n",
    "                    if key not in epoch_metrics:\n",
    "                        epoch_metrics[key] = []\n",
    "                    epoch_metrics[key].append(value)\n",
    "            \n",
    "            # Average metrics\n",
    "            avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n",
    "            history['epoch'].append(epoch)\n",
    "            history['metrics'].append(avg_metrics)\n",
    "            \n",
    "            if (epoch + 1) % log_interval == 0:\n",
    "                metric_str = ', '.join([f\"{k}: {v:.4f}\" for k, v in avg_metrics.items()])\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs} | {metric_str}\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. UNIVERSAL EVALUATOR\n",
    "# ============================================================================\n",
    "\n",
    "class UniversalEvaluator:\n",
    "    \"\"\"Universal evaluator for any RL algorithm\"\"\"\n",
    "    def __init__(self, algorithm: BaseOfflineAlgorithm, env):\n",
    "        self.algorithm = algorithm\n",
    "        self.env = env\n",
    "    \n",
    "    def evaluate(self, num_episodes: int = 10, deterministic: bool = True):\n",
    "        \"\"\"Evaluate algorithm\"\"\"\n",
    "        total_rewards = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.algorithm.select_action(state, deterministic)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "        \n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        std_reward = np.std(total_rewards)\n",
    "        \n",
    "        print(f\"\\nEvaluation ({num_episodes} episodes):\")\n",
    "        print(f\"Average Reward: {avg_reward:.2f} Â± {std_reward:.2f}\")\n",
    "        \n",
    "        return avg_reward, std_reward\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 11. USAGE EXAMPLES WITH AGENT WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import gymnasium as gym\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"UNIVERSAL OFFLINE RL FRAMEWORK WITH AGENT WRAPPER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 1: DQN Agent - Simple Usage\n",
    "    # ========================================================================\n",
    "    print(\"\\n[EXAMPLE 1] DQN Agent - Simple Training\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Collect data\n",
    "    print(\"Collecting offline data...\")\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    for _ in range(50):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(float(done))\n",
    "            state = next_state\n",
    "    \n",
    "    dataset = OfflineDataset(states, actions, rewards, next_states, dones)\n",
    "    print(f\"Collected {len(dataset)} transitions\")\n",
    "    \n",
    "    # Create agent with DQN\n",
    "    dqn_algorithm = DQNAlgorithm(state_dim, action_dim, double_q=True)\n",
    "    agent = Agent(dqn_algorithm, env=env, name=\"dqn_cartpole\")\n",
    "    \n",
    "    # Train with automatic evaluation and saving\n",
    "    agent.train(\n",
    "        dataset=dataset,\n",
    "        num_epochs=30,\n",
    "        batch_size=64,\n",
    "        log_interval=5,\n",
    "        eval_interval=10,  # Evaluate every 10 epochs\n",
    "        eval_episodes=5,\n",
    "        save_best=True\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    agent.evaluate(num_episodes=20)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 2: DDPG Agent - Advanced Usage\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[EXAMPLE 2] DDPG Agent - Advanced Usage\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    # Collect data\n",
    "    print(\"Collecting offline data...\")\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    for _ in range(50):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(float(done))\n",
    "            state = next_state\n",
    "    \n",
    "    dataset = OfflineDataset(states, actions, rewards, next_states, dones)\n",
    "    print(f\"Collected {len(dataset)} transitions\")\n",
    "    \n",
    "    # Create DDPG agent\n",
    "    ddpg_algorithm = DDPGAlgorithm(state_dim, action_dim, actor_lr=1e-3, critic_lr=1e-3)\n",
    "    agent = Agent(ddpg_algorithm, env=env, name=\"ddpg_pendulum\")\n",
    "    \n",
    "    # Train\n",
    "    agent.train(\n",
    "        dataset=dataset,\n",
    "        num_epochs=30,\n",
    "        batch_size=128,\n",
    "        log_interval=5,\n",
    "        eval_interval=10,\n",
    "        save_best=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    agent.evaluate(num_episodes=10)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 3: Agent Usage Patterns\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[EXAMPLE 3] Agent Usage Patterns\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create and train agent\n",
    "    bc_algorithm = BehavioralCloningAlgorithm(state_dim, action_dim, discrete=True)\n",
    "    agent = Agent(bc_algorithm, env=env, name=\"bc_cartpole\")\n",
    "    \n",
    "    # Reuse dataset\n",
    "    dataset = OfflineDataset(states[:500], actions[:500], rewards[:500], \n",
    "                            next_states[:500], dones[:500])\n",
    "    \n",
    "    print(\"\\n1. Training without evaluation:\")\n",
    "    agent.train(dataset, num_epochs=20, batch_size=64, log_interval=5)\n",
    "    \n",
    "    print(\"\\n2. Using agent for inference:\")\n",
    "    state, _ = env.reset()\n",
    "    for step in range(5):\n",
    "        action = agent.act(state, deterministic=True)\n",
    "        print(f\"Step {step}: state={state[:2]}, action={action}\")\n",
    "        state, _, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n3. Save and load:\")\n",
    "    agent.save(\"my_agent.pt\")\n",
    "    \n",
    "    # Create new agent and load weights\n",
    "    new_agent = Agent(\n",
    "        BehavioralCloningAlgorithm(state_dim, action_dim, discrete=True),\n",
    "        env=env,\n",
    "        name=\"loaded_agent\"\n",
    "    )\n",
    "    new_agent.load(\"my_agent.pt\")\n",
    "    \n",
    "    print(\"\\n4. Evaluate loaded agent:\")\n",
    "    new_agent.evaluate(num_episodes=5)\n",
    "    \n",
    "    print(\"\\n5. Access training history:\")\n",
    "    history = agent.get_training_history()\n",
    "    print(f\"Training ran for {len(history)} epochs\")\n",
    "    if history:\n",
    "        print(f\"Final metrics: {history[-1]['metrics']}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 4: Comparing Multiple Agents\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[EXAMPLE 4] Comparing Multiple Agents\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    algorithms = {\n",
    "        'DQN': DQNAlgorithm(state_dim, action_dim, double_q=False),\n",
    "        'DoubleDQN': DQNAlgorithm(state_dim, action_dim, double_q=True),\n",
    "        'BC': BehavioralCloningAlgorithm(state_dim, action_dim, discrete=True)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        agent = Agent(algorithm, env=env, name=name.lower())\n",
    "        agent.train(dataset, num_epochs=15, batch_size=64, log_interval=5)\n",
    "        avg_reward, std_reward = agent.evaluate(num_episodes=10, verbose=False)\n",
    "        results[name] = {'avg': avg_reward, 'std': std_reward}\n",
    "        print(f\"{name} - Reward: {avg_reward:.2f} Â± {std_reward:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:15s}: {result['avg']:6.2f} Â± {result['std']:.2f}\")\n",
    "    \n",
    "    best_agent = max(results.items(), key=lambda x: x[1]['avg'])[0]\n",
    "    print(f\"\\nğŸ† Best algorithm: {best_agent}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ALL EXAMPLES COMPLETE\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673706ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30144fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364abcc0-a7af-41ca-886f-9fc588f86c6f",
   "metadata": {},
   "source": [
    "vectorized env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0fb0b-2d43-4eae-9e14-b8fc5d980889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiTaskSimpleTelEnv(SimpleTelEnv):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def set_mu(self, new_mu: float):\n",
    "#         self.mu = new_mu\n",
    "\n",
    "# env = gym.make(\"gymnasium_env/SimpleTel-v0\", Nf=Nf, target_sequence=true_sequence, nv_max=nv_max)\n",
    "# vec_env = make_vec_env(MultiTaskSimpleTelEnv)\n",
    "# vec_env.env_method.has_attr(\"mu\")\n",
    "# vec_env.env_method(\"set_wrapper_attr\", \"mu\", .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d8adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Dict, Box, Discrete\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "# from sb3_contrib import TRPO, CrossQ, ARS, QRDQN, TQC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import optuna\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from environments import SimpleTelEnv\n",
    "from utils import save_results, load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c37188",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'SimpleTel-v0'\n",
    "\n",
    "OUTDIR = f'results/{env_name}/'\n",
    "if not os.path.exists(OUTDIR):\n",
    "    os.makedirs(OUTDIR)\n",
    "    \n",
    "SEED = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef877036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 10, 13, 3, 5, 19, 2, 6, 12, 10, 14, 8, 13, 9, 2, 1, 6, 11, 7, 0, 7, 2, 8, 11, 19, 16, 15, 4, 16, 4, 3, 0, 14, 5, 15, 9, 11, 19, 10, 8, 13, 9, 17, 12, 0, 17, 4, 12, 5, 6, 14, 15, 3, 17, 7, 18, 1, 1, 18, 18]\n"
     ]
    }
   ],
   "source": [
    "Nf = 20\n",
    "nv_max = 3\n",
    "random.seed(2)\n",
    "true_sequence = np.array([np.full(nv_max, i) for i in range(Nf)]).flatten()\n",
    "true_sequence = true_sequence.tolist()\n",
    "random.shuffle(true_sequence)\n",
    "print(true_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff42c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment passes all checks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hurra/.conda/envs/cosmoML/lib/python3.11/site-packages/gymnasium/envs/registration.py:636: UserWarning: \u001b[33mWARN: Overriding environment gymnasium_env/SimpleTel-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Register the environment so we can create it with gym.make()\n",
    "gym.register(\n",
    "    id=f\"gymnasium_env/{env_name}\",\n",
    "    entry_point=SimpleTelEnv,\n",
    "    max_episode_steps=300,  # Prevent infinite episodes. Here just set to 300 even though episode will terminate when stepping to last element of sequence\n",
    ")\n",
    "\n",
    "env = gym.make(f\"gymnasium_env/{env_name}\", Nf=Nf, target_sequence=true_sequence, nv_max=nv_max, off_by_lim=3)\n",
    "\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# This will catch many common issues\n",
    "try:\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c23b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85225be-5a9c-410c-99c2-bd7adbb52891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Q-Learning agent.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            learning_rate: How quickly to update Q-values (0-1)\n",
    "            initial_epsilon: Starting exploration rate (usually 1.0)\n",
    "            epsilon_decay: How much to reduce epsilon each episode\n",
    "            final_epsilon: Minimum exploration rate (usually 0.1)\n",
    "            discount_factor: How much to value future rewards (0-1)\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "\n",
    "        # Q-table: maps (state, action) to expected reward\n",
    "        # defaultdict automatically creates entries with zeros for new states\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor  # How much we care about future rewards\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"Choose an action using epsilon-greedy strategy.\n",
    "\n",
    "        Returns:\n",
    "            action: 0 (stand) or 1 (hit)\n",
    "        \"\"\"\n",
    "        # With probability epsilon: explore (random action)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        # With probability (1-epsilon): exploit (best known action)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Update Q-value based on experience.\n",
    "\n",
    "        This is the heart of Q-learning: learn from (state, action, reward, next_state)\n",
    "        \"\"\"\n",
    "        # What's the best we could do from the next state?\n",
    "        # (Zero if episode terminated - no future rewards possible)\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "\n",
    "        # What should the Q-value be? (Bellman equation)\n",
    "        target = reward + self.discount_factor * future_q_value\n",
    "\n",
    "        # How wrong was our current estimate?\n",
    "        temporal_difference = target - self.q_values[obs][action]\n",
    "\n",
    "        # Update our estimate in the direction of the error\n",
    "        # Learning rate controls how big steps we take\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "\n",
    "        # Track learning progress (useful for debugging)\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb4b8f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'field_id': 16,\n",
       "  'nvisits': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        dtype=int32),\n",
       "  'index': 0},\n",
       " {'chosen_field_id': None, 'correct': None})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af0649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'field_id': np.int64(12),\n",
       " 'index': np.int64(41),\n",
       " 'nvisits': array([4, 2, 0, 4, 3, 2, 2, 2, 3, 1, 4, 2, 1, 4, 0, 2, 1, 4, 2, 3],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f94c55bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QLearningAgent.update() missing 5 required positional arguments: 'obs', 'action', 'reward', 'terminated', and 'next_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: QLearningAgent.update() missing 5 required positional arguments: 'obs', 'action', 'reward', 'terminated', and 'next_obs'"
     ]
    }
   ],
   "source": [
    "agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de61185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.01        # How fast to learn (higher = faster but less stable)\n",
    "n_episodes = 100_000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
    "final_epsilon = 0.1         # Always keep some exploration\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0e036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m next_obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Learn from this experience\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Move to next state\u001b[39;00m\n\u001b[32m     20\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mQLearningAgent.update\u001b[39m\u001b[34m(self, obs, action, reward, terminated, next_obs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Update Q-value based on experience.\u001b[39;00m\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m \u001b[33;03mThis is the heart of Q-learning: learn from (state, action, reward, next_state)\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# What's the best we could do from the next state?\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# (Zero if episode terminated - no future rewards possible)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m future_q_value = (\u001b[38;5;129;01mnot\u001b[39;00m terminated) * np.max(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# What should the Q-value be? (Bellman equation)\u001b[39;00m\n\u001b[32m     74\u001b[39m target = reward + \u001b[38;5;28mself\u001b[39m.discount_factor * future_q_value\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    # Start a new hand\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Play one complete hand\n",
    "    while not done:\n",
    "        # Agent chooses action (initially random, gradually more intelligent)\n",
    "        action = agent.get_action(obs)\n",
    "\n",
    "        # Take action and observe result\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Learn from this experience\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # Move to next state\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    # Reduce exploration rate (agent becomes less random over time)\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_dataset(num_episodes=100_000):\n",
    "#     grid_max = 10\n",
    "#     ra_range = (-grid_max, grid_max)\n",
    "#     dec_range = (-grid_max, grid_max)\n",
    "#     n_points = grid_max\n",
    "#     nvisits = 1\n",
    "    \n",
    "#     # generate random coords\n",
    "#     ra_list = np.random.randint(ra_range[0], ra_range[1], size=(num_episodes, n_points))\n",
    "#     dec_list = np.random.randint(dec_range[0], dec_range[1], size=(num_episodes, n_points))\n",
    "#     coords = np.stack([ra_list, dec_list], axis=2) # shape (num_ep, nra_points, ndec_points)\n",
    "#     coords_dict = {f'eps{i}': coord for i, coord in enumerate(coords)}\n",
    "    \n",
    "#     distance_matrices = np.empty(shape=(num_episodes, grid_max, grid_max))\n",
    "#     # get distance matrices\n",
    "#     for i in range(num_episodes):\n",
    "#         distance_matrices[i] = get_distance_matrix(coords[i], coords[i])\n",
    "#         np.fill_diagonal(distance_matrices[i], np.inf)\n",
    "        \n",
    "#     ordered_indices = np.argsort(distance_matrices, axis=2)\n",
    "#     return coords, ordered_indices\n",
    "\n",
    "#     start_ind = np.argmin(np.sum(coords**2, axis=2), axis=1)\n",
    "    \n",
    "#     target_indices = np.full(shape=(num_episodes, n_points), fill_value=-1, dtype=np.int32)\n",
    "#     target_indices[:, 0] = start_ind\n",
    "\n",
    "#     last_ind = start_ind\n",
    "#     for i in range(num_episodes):\n",
    "#         for k in range(len(coords[i]) - 1):\n",
    "#             j = 0\n",
    "#             current_ind = ordered_indices[i][last_ind[i]][j]\n",
    "#             while current_ind in target_indices[i]:\n",
    "#                 j += 1\n",
    "#                 current_ind = ordered_indices[i][last_ind[i]][j]\n",
    "#             target_indices[i, k] = current_ind\n",
    "#             last_ind[i] = current_ind\n",
    "#     return target_indices, coords\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
